<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
	
  <title>Guihong Li</title>
  <meta name="author" content="Guihong Li">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta http-equiv='cache-control' content='no-cache'> 
  <meta http-equiv='expires' content='0'> 
  <meta http-equiv='pragma' content='no-cache'>
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
	<link rel="icon" type="image/png" href="libs/icon.png">
  <link href="https://maxcdn.bootstrapcdn.com/font-awesome/4.2.0/css/font-awesome.min.css" rel="stylesheet">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/academicons/1.9.0/css/academicons.min.css" integrity="sha512-W4yqoT1+8NLkinBLBZko+dFB2ZbHsYLDdr50VElllRcNt2Q4/GSs6u71UHKxB7S6JEMCp5Ve4xjh3eGQl/HRvg==" crossorigin="anonymous">
  <script type="text/javascript"> 
	  function play(audio_id){
		  var audio = document.getElementById(audio_id);
		  audio.play();
	  }
	  function showHide(shID) {
		  if (document.getElementById(shID)) {
			  if (document.getElementById(shID).style.display == 'none') {
				  document.getElementById(shID).style.display = 'inline';
			  }
			  else {
				  document.getElementById(shID).style.display = 'inline';
				  document.getElementById(shID).style.display = 'none';
			  }
		  }
	  }
  </script>
</head>

	
<body>
	
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Guihong Li</name>
              </p>
              <p>
                I'm a MTS (Member of Technical Staff) at <a href="https://www.amd.com/">AMD</a> where I focus on efficient AI system design. Before joining AMD, I obtained my Ph.D. degree from <a href="https://www.utexas.edu/">UT Austin</a> in May 2024.
              </p>
              <p>
                My current research interests include:
              </p>
              <ul>
                <li><b>Efficient Generative AI Models Design</b>: I am doing the research to build the next-gen of model architecture to accelerate inference throughput, including foundation LLMs and diffusion models.</li>
                <li><b>Efficient Training Systems</b>: I am building efficient, reliable and large-scale data centers to enable the training of large foundation AI models. My work focuses on improving training performance on Megatron-LM, JAX, and native PyTorch on AMD hardware platforms.</li>
              </ul>
              <p style="text-align:center">
                <a href="mailto:guihong.li@amd.com">Email</a> &nbsp/&nbsp
                <a href="https://scholar.google.com/citations?hl=en&user=UJofPMIAAAAJ&view_op=list_works&sortby=pubdate">Google Scholar</a> &nbsp/&nbsp
                <a href="https://twitter.com/KingLGH">Twitter</a> &nbsp/&nbsp
                <a href="https://www.linkedin.com/in/guihong-li-694144126/">LinkedIn</a>
              </p>
            </td>
            <td style="padding:2.5%;width:40%;max-width:40%;text-align:center;vertical-align:middle">
              <img src="assets/profile-pics/avatar.jpg" style="width:100%;max-width:100%;border-radius:50%;" alt="profile photo">
            </td>
          </tr>
		
	</tbody>
        </table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>News</heading>
              <p>
                <ul>
                  <li style="list-style-position:inside;margin:0;padding:0;text-align:left;"><b>09-2025</b>: One paper accepted to <a href="https://neurips.cc/">NeurIPS 2025</a></li>
                  <li style="list-style-position:inside;margin:0;padding:0;text-align:left;"><b>09-2025</b>: We developed, tested and delivered the scalable and reliable large-scale MI300X GPU training systems to our customer (<a href="https://www.amd.com/en/newsroom/press-releases/2025-9-24-amd-and-cohere-expand-global-ai-collaboration-to-p.html" target="_blank">blog</a>)!</li>
                  <li style="list-style-position:inside;margin:0;padding:0;text-align:left;"><b>09-2025</b>: We trained and open-sourced AMD's first hybrid models (combine linear attention and multi-head attention) and it's highlighted on AMD website (<a href="https://rocm.blogs.amd.com/artificial-intelligence/hybrid-models,-mla,/README.html" target="_blank">blog</a>)!</li>
                  <li style="list-style-position:inside;margin:0;padding:0;text-align:left;"><b>05-2025</b>: One paper accepted to COLM 2025</li>
                  <li style="list-style-position:inside;margin:0;padding:0;text-align:left;"><b>03-2025</b>: One paper accepted to <a href="https://cvpr.thecvf.com/">CVPR 2025</a></li>
                  <li style="list-style-position:inside;margin:0;padding:0;text-align:left;"><b>02-2025</b>: AMD released the first version of unified training docker; I am proud that I proposed this idea and was deeply involved on the development stages (<a href="https://rocm.blogs.amd.com/software-tools-optimization/amd-optimized-rocm-docker-for-distributed-training/README.html" target="_blank">blog</a>)</li>
                  <li style="list-style-position:inside;margin:0;padding:0;text-align:left;"><b>01-2025</b>: We developed and tested the efficient fine-tuning recipe on AMD MI300X and tested on OCI</li>
                  <li style="list-style-position:inside;margin:0;padding:0;text-align:left;"><b>11-2024</b>: AMD released the new AMD MI325X GPU and RoCm-6.2; I am proud that I was deeply involved on both release (<a href="https://ir.amd.com/news-events/press-releases/detail/1218/amd-unveils-leadership-ai-solutions-at-advancing-ai-2024" target="_blank">blog</a>)</li>
                  <li style="list-style-position:inside;margin:0;padding:0;text-align:left;"><b>11-2024</b>: One paper accepted to <a href="https://wacv2025.thecvf.com/">WACV 2025</a></li>
                  <li style="list-style-position:inside;margin:0;padding:0;text-align:left;"><b>03-2024</b>: One paper accepted to T-PAMI</li>
                  <li style="list-style-position:inside;margin:0;padding:0;text-align:left;"><b>03-2024</b>: Two papers accepted to <a href="https://cvpr.thecvf.com/">CVPR 2024</a></li>
                  <li style="list-style-position:inside;margin:0;padding:0;text-align:left;"><b>01-2024</b>: Two papers accepted to <a href="https://iclr.cc/">ICLR 2024</a></li>
                  <li style="list-style-position:inside;margin:0;padding:0;text-align:left;"><b>09-2023</b>: One paper accepted to <a href="https://neurips.cc/">NeurIPS 2023</a></li>
                  <li style="list-style-position:inside;margin:0;padding:0;text-align:left;"><b>05-2023</b>: One paper accepted to <a href="https://icml.cc/">ICML 2023</a></li>
                  <li style="list-style-position:inside;margin:0;padding:0;text-align:left;"><b>02-2023</b>: One paper accepted to <a href="https://cvpr.thecvf.com/">CVPR 2023</a></li>
                  <li style="list-style-position:inside;margin:0;padding:0;text-align:left;"><b>01-2023</b>: One paper accepted to <a href="https://iclr.cc/">ICLR 2023</a> as <b><font color="red">Spotlight</font></b></li>
                </ul>
              </p>
            </td>
          </tr>
		
	  
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Professional Experience</heading>
            </td>
          </tr>
        </tbody></table>
        
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <p>
                <b>Member of Technical Staff @ AMD</b>
                <br>
                <i>Bellevue, June 2024 - Now</i>
              </p>
              <ul>
                <li><b>Efficient Generative AI Models Design</b>: Improve the inference efficiency for high-cost generative AI models, including large language models and diffusion models.</li>
                <li><b>Efficient Training Systems</b>: Build the efficient large-scale data centers to enable the training of large foundation AI models.</li>
              </ul>
              
              <p>
                <b>Applied Scientist Intern @ JPMorgan Chase & Co</b>
                <br>
                <i>New York, June 2023 - October 2023</i>
                <br>
                Mentors: Dr. Richard Chun-Fu Chen, Dr. Hsiang Hsu
              </p>
              <ul>
                <li><b>Trustworthy Generative models</b>: Control the contents generated by image generative models.</li>
                <li><b>Efficient Machine Unlearning</b>: Build an efficient machine unlearning algorithm to quickly remove information from a trained model.</li>
              </ul>
              
              <p>
                <b>Research Scientist Intern @ ARM ML Tech</b>
                <br>
                <i>San Jose, May 2021 - August 2021</i>
                <br>
                Mentors: Dr. Kartikeya Bhardwaj, Dr. Naveen Suda, Dr. Lingchuan Meng
              </p>
              <ul>
                <li><b>Hardware-aware NAS</b>: Explored the neural architecture search technique to search for hardware-efficient models.</li>
                <li><b>Hardware Performance evaluation</b>: Built a model to estimate neural networks' latency on neural accelerators.</li>
              </ul>
            </td>
          </tr>
        </tbody></table>
        
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Publications (Selected)</heading>
              <p>Full publications on my <a href="https://scholar.google.com/citations?hl=en&user=UJofPMIAAAAJ&view_op=list_works&sortby=pubdate" target="_blank">Google Scholar</a>.</p>
            </td>
          </tr>
		
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

          <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <papertitle>Zebra-Llama: Towards Extremely Efficient Hybrid Models</papertitle>
              <br>
              Mingyu Yang*, Mehdi Rezagholizadeh*, <b>Guihong Li</b>*, Vikram Appia, and Emad Barsoum. (*Equal contribution)
              <br>
              <em>NeurIPS</em>, 2025
              <br>
              <a href="https://openreview.net/pdf?id=9hjVoPWPnh">paper</a>
              <p></p>
            </td>
          </tr>
          
          <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <papertitle>X-EcoMLA: Upcycling Pre-Trained Attention into MLA for Efficient and Extreme KV Compression</papertitle>
              <br>
              <b>Guihong Li</b>*, Mehdi Rezagholizadeh*, Mingyu Yang*, Vikram Appia, and Emad Barsoum. (*Equal contribution)
              <br>
              <em>COLM</em>, 2025
              <br>
              <a href="https://openreview.net/pdf?id=CPJ9EAeYfd">paper</a>
              <p></p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <papertitle>Zero-Shot Neural Architecture Search: Challenges, Solutions, and Opportunities</papertitle>
              <br>
              <b>Guihong Li</b>*, Duc Hoang, Kartikeya Bhardwaj, Ming Lin, Zhangyang Wang, Radu Marculescu. (*equal contribution)
              <br>
              <em>IEEE T-PAMI</em>, 2024
              <br>
              <a href="https://arxiv.org/pdf/2307.01998">paper</a>
              <p></p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <papertitle>Machine Unlearning for Image-to-Image Generative Models</papertitle>
              <br>
              <b>Guihong Li</b>, Hsiang Hsu, Chun-Fu Chen, Radu Marculescu
              <br>
              <em>ICLR</em>, 2024
              <br>
              <a href="https://openreview.net/pdf?id=9hjVoPWPnh">paper</a>
              <p></p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <papertitle>Dropout-Based Rashomon Set Exploration for Efficient Predictive Multiplicity Estimation</papertitle>
              <br>
              Hsiang Hsu, <b>Guihong Li</b>, Shaohan Hu, Chun-Fu Chen
              <br>
              <em>ICLR</em>, 2024
              <br>
              <a href="https://openreview.net/pdf?id=Sf2A2PUXO3">paper</a>
              <p></p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <papertitle>Efficient Low-rank Backpropagation for Vision Transformer Adaptation</papertitle>
              <br>
              Yuedong Yang, Hung-Yueh Chiang, <b>Guihong Li</b>, Diana Marculescu, Radu Marculescu
              <br>
              <em>NeurIPS</em>, 2023
              <br>
              <a href="https://arxiv.org/pdf/2309.15275.pdf">paper</a>
              <p></p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <papertitle>TIPS: Topologically Important Path Sampling for Anytime Neural Networks</papertitle>
              <br>
              <b>Guihong Li</b>, Kartikeya Bhardwaj, Yuedong Yang, Radu Marculescu
              <br>
              <em>ICML</em>, 2023
              <br>
              <a href="https://arxiv.org/pdf/2305.08021.pdf">paper</a>
              <p></p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <papertitle>Efficient On-device Training via Gradient Filtering</papertitle>
              <br>
              Yuedong Yang, <b>Guihong Li</b>, Radu Marculescu
              <br>
              <em>CVPR</em>, 2023
              <br>
              <a href="https://arxiv.org/pdf/2301.00330.pdf">paper</a>
              <p></p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <papertitle>ZiCo: Zero-shot NAS via inverse Coefficient of Variation on Gradients</papertitle>
              <br>
              <b>Guihong Li</b>, Yuedong Yang, Kartikeya Bhardwaj, Radu Marculescu
              <br>
              <em>ICLR</em>, 2023 &nbsp <font color="red"><b>(Spotlight)</b></font>
              <br>
              <a href="https://openreview.net/pdf?id=rwo-ls5GqGn">paper</a>
              <p></p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <papertitle>How does topology influence gradient propagation and model performance of deep networks with densenet-type skip connections?</papertitle>
              <br>
              Kartikeya Bhardwaj*, <b>Guihong Li</b>*, Radu Marculescu. (*Equal contribution)
              <br>
              <em>CVPR</em>, 2021
              <br>
              <a href="https://openaccess.thecvf.com/content/CVPR2021/papers/Bhardwaj_How_Does_Topology_Influence_Gradient_Propagation_and_Model_Performance_of_CVPR_2021_paper.pdf">paper</a>
              <p></p>
            </td>
          </tr>

        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:0px">
              <br>
              <p style="text-align:right;float:right;font-size:small;align-items:center;">
                Website template credit to <a href="https://jonbarron.info/" target="_blank">Dr. Jon Barron</a>
                <br>
                Last updated: December 2025
              </p>
            </td>
          </tr>
        </tbody></table>
      </td>
    </tr>
  </table>
</body>

</html>
