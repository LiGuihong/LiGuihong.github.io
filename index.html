<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
	
  <title>Guihong Li</title>
  <meta name="author" content="Guihong Li">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta http-equiv='cache-control' content='no-cache'> 
  <meta http-equiv='expires' content='0'> 
  <meta http-equiv='pragma' content='no-cache'>
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
	<link rel="icon" type="image/png" href="libs/icon.png">
  <link href="https://maxcdn.bootstrapcdn.com/font-awesome/4.2.0/css/font-awesome.min.css" rel="stylesheet">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/academicons/1.9.0/css/academicons.min.css" integrity="sha512-W4yqoT1+8NLkinBLBZko+dFB2ZbHsYLDdr50VElllRcNt2Q4/GSs6u71UHKxB7S6JEMCp5Ve4xjh3eGQl/HRvg==" crossorigin="anonymous">
  <script type="text/javascript"> 
	  function play(audio_id){
		  var audio = document.getElementById(audio_id);
		  audio.play();
	  }
	  function showHide(shID) {
		  if (document.getElementById(shID)) {
			  if (document.getElementById(shID).style.display == 'none') {
				  document.getElementById(shID).style.display = 'inline';
			  }
			  else {
				  document.getElementById(shID).style.display = 'inline';
				  document.getElementById(shID).style.display = 'none';
			  }
		  }
	  }
  </script>
</head>

	
<body>

  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Guihong Li</name>
              </p>
              <p>
                I'm a MTS (Member of Technical Staff) at <a href="https://www.amd.com/">AMD</a> where I focus on efficient Generative AI models and large-scale training systems. I received my Ph.D. from <a href="https://www.utexas.edu/">UT Austin</a> in May 2024. My research has been published at top-tier venues including <b>NeurIPS</b>, <b>COLM</b>, <b>ICLR</b>, <b>ICML</b>, and <b>CVPR</b>.
              </p>
              <p>
                <b>Research Focus</b>: Building next-generation efficient LLM architectures and scalable training infrastructure for foundation models. I specialize in:
              </p>
              <ul>
                <li><b>Efficient LLM Architectures</b>: Developed hybrid models achieving up to <b>50x KV cache compression</b> and <b>3.6x inference speedup</b> through novel attention mechanisms (MLA, Mamba) and distillation strategies.</li>
                <li><b>Large-Scale Training Systems</b>: Optimized distributed training with various parallelism strategies (DP, FSDP, EP) for dense and MoE models across <b>PyTorch, JAX, and Megatron-LM</b>. Delivered production-ready training systems for enterprise customers.</li>
              </ul>
              <p style="text-align:center">
                <a href="mailto:guihong.li@amd.com">Email</a> &nbsp/&nbsp
                <a href="https://scholar.google.com/citations?hl=en&user=UJofPMIAAAAJ&view_op=list_works&sortby=pubdate">Google Scholar</a> &nbsp/&nbsp
                <a href="https://twitter.com/KingLGH">Twitter</a> &nbsp/&nbsp
                <a href="https://www.linkedin.com/in/guihong-li-694144126/">LinkedIn</a>
              </p>
            </td>
            <td style="padding:2.5%;width:40%;max-width:40%;text-align:center;vertical-align:middle">
              <img src="assets/profile-pics/avatar.jpg" style="width:100%;max-width:100%;border-radius:50%;" alt="profile photo">
            </td>
          </tr>
		
	</tbody>
        </table>
        
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Key Achievements</heading>
              <div class="achievement-box">
                <ul style="margin:0;">
                  <li style="list-style-position:inside;margin:5px 0;padding:0;text-align:left;"><b>Zebra-Llama</b> (NeurIPS 2025): AMD's hybrid MLA-Mamba LLM achieving <b>50x KV cache compression</b> and <b>3.6x inference speedup</b> via novel 3-stage distillation strategy.</li>
                  <li style="list-style-position:inside;margin:5px 0;padding:0;text-align:left;"><b>X-EcoMLA</b> (COLM 2025): Upcycled GQA/MHA to MLA achieving <b>12.8x KV compression</b> and <b>2x inference speedup</b> with minimal quality loss.</li>
                  <li style="list-style-position:inside;margin:5px 0;padding:0;text-align:left;"><b>Production Impact</b>: Delivered scalable <b>MI300X GPU training systems</b> to enterprise customers; led development of AMD's unified training docker.</li>
                  <li style="list-style-position:inside;margin:5px 0;padding:0;text-align:left;"><b>Research Contributions</b>: Published <b>15+ papers</b> at top-tier venues (NeurIPS, ICLR, ICML, CVPR) with focus on efficient architectures and training systems.</li>
        </ul>
      </div>
            </td>
          </tr>
        </tbody></table>
        
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>News</heading>
              <p>
                <ul>
                  <li style="list-style-position:inside;margin:0;padding:0;text-align:left;"><b>09-2025</b>: One paper accepted to NeurIPS 2025</li>
                  <li style="list-style-position:inside;margin:0;padding:0;text-align:left;"><b>09-2025</b>: We developed, tested and delivered the scalable and reliable large-scale MI300X GPU training systems to our customer (<a href="https://www.amd.com/en/newsroom/press-releases/2025-9-24-amd-and-cohere-expand-global-ai-collaboration-to-p.html" target="_blank">blog</a>)!</li>
                  <li style="list-style-position:inside;margin:0;padding:0;text-align:left;"><b>09-2025</b>: We trained and open-sourced AMD's first hybrid models (combine linear attention and multi-head attention) and it's highlighted on AMD website (<a href="https://rocm.blogs.amd.com/artificial-intelligence/hybrid-models,-mla,/README.html" target="_blank">blog</a>)!</li>
                  <li style="list-style-position:inside;margin:0;padding:0;text-align:left;"><b>05-2025</b>: One paper accepted to CoLM 2025</li>
                  <li style="list-style-position:inside;margin:0;padding:0;text-align:left;"><b>03-2025</b>: One paper accepted to CVPR 2025</li>
                  <li style="list-style-position:inside;margin:0;padding:0;text-align:left;"><b>03-2025</b>: AMD released the first version of unified training docker; I proposed this idea and was deeply involved on the development stages (<a href="https://rocm.blogs.amd.com/software-tools-optimization/amd-optimized-rocm-docker-for-distributed-training/README.html" target="_blank">blog</a>)</li>
                  <li style="list-style-position:inside;margin:0;padding:0;text-align:left;"><b>10-2024</b>: AMD released the new AMD MI325X GPU and RoCm-6.2; I was deeply involved on both release (<a href="https://ir.amd.com/news-events/press-releases/detail/1218/amd-unveils-leadership-ai-solutions-at-advancing-ai-2024" target="_blank">blog</a>)!</li>
                  <li style="list-style-position:inside;margin:0;padding:0;text-align:left;"><b>11-2024</b>: One paper accepted to WACV 2025</li>
                  <li style="list-style-position:inside;margin:0;padding:0;text-align:left;"><b>03-2024</b>: One paper accepted to T-PAMI</li>
                  <li style="list-style-position:inside;margin:0;padding:0;text-align:left;"><b>03-2024</b>: Two papers accepted to CVPR 2024</li>
                  <li style="list-style-position:inside;margin:0;padding:0;text-align:left;"><b>01-2024</b>: Two papers accepted to ICLR 2024</li>
                  <li style="list-style-position:inside;margin:0;padding:0;text-align:left;"><b>09-2023</b>: One paper accepted to NeurIPS 2023</li>
                  <li style="list-style-position:inside;margin:0;padding:0;text-align:left;"><b>05-2023</b>: One paper accepted to ICML 2023</li>
                  <li style="list-style-position:inside;margin:0;padding:0;text-align:left;"><b>02-2023</b>: One paper accepted to CVPR 2023</li>
                  <li style="list-style-position:inside;margin:0;padding:0;text-align:left;"><b>01-2023</b>: One paper accepted to ICLR 2023 as <b><font color="red">Spotlight</font></b></li>
    </ul>
  </p>
            </td>
          </tr>
		
	  
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Professional Experience</heading>
            </td>
          </tr>
        </tbody></table>
        
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <p>
                <b>Member of Technical Staff @ AMD AI Group</b>
                <br>
                <i>Bellevue, WA Â· June 2024 - Present</i>
              </p>
              <ul>
                <li><b>Efficient LLM Architectures</b>: Developed Zebra-Llama (50x KV compression, 3.6x speedup) and X-EcoMLA (12.8x compression, 2x speedup) - AMD's first hybrid models combining MLA and Mamba architectures. Research on autoregressive to block diffusion adaptation and byte-level LLM distillation.</li>
                <li><b>Large-Scale Training Optimization</b>: Performance analysis and optimization for distributed training (DP, FSDP, EP) on dense and MoE models in PyTorch and JAX. Delivered production MI300X training systems to enterprise customers.</li>
                <li><b>Infrastructure & Tools</b>: Proposed and led development of AMD's unified training docker. Built efficient fine-tuning recipes for MI300X. Contributed to MI325X GPU and RoCm-6.2 releases.</li>
    </ul>
  <p>
                <b>Applied Scientist Intern @ JPMorgan Chase & Co</b>
                <br>
    <i>New York, June 2023 - October 2023</i>
                <br>
    Mentors: Dr. Richard Chun-Fu Chen, Dr. Hsiang Hsu
              </p>
    <ul>
                <li><b>Trustworthy Generative models</b>: Control the contents generated by image generative models.</li>
                <li><b>Efficient Machine Unlearning</b>: Build an efficient machine unlearning algorithm to quickly remove information from a trained model.</li>
    </ul>
  <p>
                <b>Research Scientist Intern @ ARM ML Tech</b>
                <br>
    <i>San Jose, May 2021 - August 2021</i>
                <br>
    Mentors: Dr. Kartikeya Bhardwaj, Dr. Naveen Suda, Dr. Lingchuan Meng
              </p>
    <ul>
                <li><b>Hardware-aware NAS</b>: Explored the neural architecture search technique to search for hardware-efficient models.</li>
                <li><b>Hardware Performance evaluation</b>: Built a model to estimate neural networks' latency on neural accelerators.</li>
    </ul>
            </td>
          </tr>
        </tbody></table>
        
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Publications (Selected)</heading>
              <p>Full publications on my <a href="https://scholar.google.com/citations?hl=en&user=UJofPMIAAAAJ&view_op=list_works&sortby=pubdate" target="_blank">Google Scholar</a>.</p>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <papertitle>Zebra-Llama: Towards Extremely Efficient Hybrid Models</papertitle>
              <br>
              Mingyu Yang*, Mehdi Rezagholizadeh*, <b>Guihong Li</b>*, Vikram Appia, and Emad Barsoum. (*Equal contribution)
              <br>
              <em>NeurIPS</em>, 2025
              <br>
              <a href="https://openreview.net/pdf?id=9hjVoPWPnh">paper</a>
              <p style="margin-top:5px;"><i>AMD's first hybrid MLA-Mamba LLM achieving 50x KV cache compression and 3.6x inference speedup through novel 3-stage distillation and layer selection strategy.</i></p>
            </td>
          </tr>
          <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <papertitle>X-EcoMLA: Upcycling Pre-Trained Attention into MLA for Efficient and Extreme KV Compression</papertitle>
              <br>
              <b>Guihong Li</b>*, Mehdi Rezagholizadeh*, Mingyu Yang*, Vikram Appia, and Emad Barsoum. (*Equal contribution)
              <br>
              <em>COLM</em>, 2025
              <br>
              <a href="https://openreview.net/pdf?id=CPJ9EAeYfd">paper</a>
              <p style="margin-top:5px;"><i>Novel technique for upcycling GQA/MHA modules to Multi-Latent Attention, achieving 12.8x KV cache compression and 2x inference speedup with minimal quality degradation.</i></p>
            </td>
          </tr>
          <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <papertitle>Zero-Shot Neural Architecture Search: Challenges, Solutions, and Opportunities</papertitle>
              <br>
              <b>Guihong Li</b>, Duc Hoang, Kartikeya Bhardwaj, Ming Lin, Zhangyang Wang, Radu Marculescu.
              <br>
              <em>IEEE T-PAMI</em>, 2024
              <br>
              <a href="https://arxiv.org/pdf/2307.01998">paper</a>
              <p></p>
            </td>
          </tr>
          <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <papertitle>Machine Unlearning for Image-to-Image Generative Models</papertitle>
              <br>
              <b>Guihong Li</b>, Hsiang Hsu, Chun-Fu Chen, Radu Marculescu
              <br>
              <em>ICLR</em>, 2024
              <br>
              <a href="https://openreview.net/pdf?id=9hjVoPWPnh">paper</a>
              <p style="margin-top:5px;"><i>Efficient machine unlearning algorithm to quickly remove information from trained generative models, addressing trustworthiness and data privacy concerns.</i></p>
            </td>
          </tr>
          <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <papertitle>Dropout-Based Rashomon Set Exploration for Efficient Predictive Multiplicity Estimation</papertitle>
              <br>
              Hsiang Hsu, <b>Guihong Li</b>, Shaohan Hu, Chun-Fu Chen
              <br>
              <em>ICLR</em>, 2024
              <br>
              <a href="https://openreview.net/pdf?id=Sf2A2PUXO3">paper</a>
              <p></p>
            </td>
          </tr>
          <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <papertitle>Efficient Low-rank Backpropagation for Vision Transformer Adaptation</papertitle>
              <br>
              Yuedong Yang, Hung-Yueh Chiang, <b>Guihong Li</b>, Diana Marculescu, Radu Marculescu
              <br>
              <em>NeurIPS</em>, 2023
              <br>
              <a href="https://arxiv.org/pdf/2309.15275.pdf">paper</a>
              <p></p>
            </td>
          </tr>
          <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <papertitle>TIPS: Topologically Important Path Sampling for Anytime Neural Networks</papertitle>
              <br>
              <b>Guihong Li</b>, Kartikeya Bhardwaj, Yuedong Yang, Radu Marculescu
              <br>
              <em>ICML</em>, 2023
              <br>
              <a href="https://arxiv.org/pdf/2305.08021.pdf">paper</a>
              <p></p>
            </td>
          </tr>
          <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <papertitle>Efficient On-device Training via Gradient Filtering</papertitle>
              <br>
              Yuedong Yang, <b>Guihong Li</b>, Radu Marculescu
              <br>
              <em>CVPR</em>, 2023
              <br>
              <a href="https://arxiv.org/pdf/2301.00330.pdf">paper</a>
              <p></p>
            </td>
          </tr>
          <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <papertitle>ZiCo: Zero-shot NAS via inverse Coefficient of Variation on Gradients</papertitle>
              <br>
              <b>Guihong Li</b>, Yuedong Yang, Kartikeya Bhardwaj, Radu Marculescu
              <br>
              <em>ICLR</em>, 2023 &nbsp <font color="red"><b>(Spotlight)</b></font>
              <br>
              <a href="https://openreview.net/pdf?id=rwo-ls5GqGn">paper</a>
              <p style="margin-top:5px;"><i>Zero-shot neural architecture search using gradient variation analysis, enabling efficient model discovery without training.</i></p>
            </td>
          </tr>
          <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <papertitle>How does topology influence gradient propagation and model performance of deep networks with densenet-type skip connections?</papertitle>
              <br>
              Kartikeya Bhardwaj*, <b>Guihong Li</b>*, Radu Marculescu. (*Equal contribution)
              <br>
              <em>CVPR</em>, 2021
              <br>
              <a href="https://openaccess.thecvf.com/content/CVPR2021/papers/Bhardwaj_How_Does_Topology_Influence_Gradient_Propagation_and_Model_Performance_of_CVPR_2021_paper.pdf">paper</a>
              <p></p>
            </td>
          </tr>
        </tbody></table>
        
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Technical Expertise</heading>
              <p>
                <b>LLM Architectures</b>: Multi-Latent Attention (MLA), Mamba, GQA, MHA, Transformers, MoE models, Diffusion models
                <br>
                <b>Training Frameworks</b>: PyTorch, JAX, Megatron-LM, DeepSpeed, FSDP
                <br>
                <b>Distributed Training</b>: Data Parallelism (DP), Fully Sharded Data Parallelism (FSDP), Expert Parallelism (EP), Tensor Parallelism (TP)
                <br>
                <b>Optimization</b>: Knowledge distillation, Model compression, Quantization, Efficient fine-tuning (LoRA, Adapters)
                <br>
                <b>Hardware</b>: AMD MI300X, MI325X, NVIDIA GPUs, TPUs
                <br>
                <b>Systems</b>: Docker, Kubernetes, SLURM, RoCm, CUDA
              </p>
            </td>
          </tr>
        </tbody></table>
        
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:0px">
              <br>
              <p style="text-align:right;float:right;font-size:small;align-items:center;">
                Website template credit to <a href="https://jonbarron.info/" target="_blank">Dr. Jon Barron</a>
                <br>
                Last updated: December 2025
              </p>
            </td>
          </tr>
        </tbody></table>
      </td>
    </tr>
  </table>
</body>

</html>
